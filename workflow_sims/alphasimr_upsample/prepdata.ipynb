{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import h5py\n",
    "import gc\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "######################################################################################\n",
    "\n",
    "def save_to_hdf5(data_input: dict, hdf5_path: Path, gzip: bool = True) -> Path:\n",
    "    data = data_input\n",
    "    str_dt = h5py.string_dtype(encoding=\"utf-8\")\n",
    "\n",
    "    with h5py.File(hdf5_path, \"w\") as h5f:\n",
    "        metadata_group = h5f.create_group(\"metadata\")\n",
    "\n",
    "        loci_array = np.array(data[\"loci\"], dtype=str_dt)\n",
    "        metadata_group.create_dataset(\"loci\", data=loci_array)\n",
    "\n",
    "        pheno_names_array = np.array(data[\"phenotype_names\"], dtype=str_dt)\n",
    "        metadata_group.create_dataset(\"phenotype_names\", data=pheno_names_array)\n",
    "\n",
    "        strains_group = h5f.create_group(\"strains\")\n",
    "\n",
    "        for idx, strain_id in enumerate(data[\"strain_names\"]):\n",
    "            strain_grp = strains_group.create_group(strain_id)\n",
    "\n",
    "            pheno = np.array(data[\"phenotypes\"][idx], dtype=np.float64)\n",
    "            strain_grp.create_dataset(\"phenotype\", data=pheno)\n",
    "\n",
    "            genotype = np.array(data[\"genotypes\"][idx], dtype=np.int8)\n",
    "            strain_grp.create_dataset(\n",
    "                \"genotype\",\n",
    "                data=genotype,\n",
    "                chunks=True,\n",
    "                compression=\"gzip\" if gzip else None,\n",
    "            )\n",
    "\n",
    "        print(f\"{hdf5_path} generated from {data_input}.\")\n",
    "\n",
    "    return hdf5_path\n",
    "out_dict={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "######################################################################################\n",
    "######################################################################################\n",
    "# Read phenotype file\n",
    "phen_file = open('test_sim_WF_1kbt_100kups_5mb_p.txt', 'r')\n",
    "phens = phen_file.read().split('\\n')\n",
    "phens = [x.split() for x in phens if x]  # Skip empty lines\n",
    "\n",
    "# Extract phenotype information\n",
    "out_dict = {}\n",
    "out_dict['phenotype_names'] = phens[0][1:]  # Extract header of pheno names from first row\n",
    "out_dict['strain_names'] = [x[0] for x in phens[1:] if x]  # Strain names from first column\n",
    "out_dict['phenotypes'] = []\n",
    "\n",
    "# Convert phenotypes to float, handling NA values\n",
    "for x in phens[1:]:\n",
    "    if not x:  # Skip empty lines\n",
    "        continue\n",
    "    row_phenos = []\n",
    "    for y in x[1:]:\n",
    "        if y == 'NA':\n",
    "            row_phenos.append(0)  # Or use None/np.nan if preferred\n",
    "        else:\n",
    "            row_phenos.append(float(y))\n",
    "    out_dict['phenotypes'].append(row_phenos)\n",
    "\n",
    "phen_file.close()\n",
    "del phens\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_dict['phenotypes'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read genotype file - CSV format with headers\n",
    "genotype_file = open('test_sim_WF_1kbt_100kups_5mb_g.txt', 'r')\n",
    "\n",
    "#Read and process header line for locus names\n",
    "header = genotype_file.readline().strip()\n",
    "out_dict['loci'] = header.split(',')\n",
    "\n",
    "# Create a temporary directory for chunk files\n",
    "temp_dir = \"temp_genotype_chunks\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# Process genotypes in chunks\n",
    "new_coding_dict = {'0': [1, 0], '1': [0, 1]}\n",
    "chunk_size = 10000  # Adjust based on your memory constraints\n",
    "chunk_files = []\n",
    "current_chunk = []\n",
    "chunk_counter = 0\n",
    "\n",
    "for line in genotype_file:\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "\n",
    "    parts = line.split(',')\n",
    "    if len(parts) <= 1:\n",
    "        continue\n",
    "\n",
    "    # Process genotypes for this individual\n",
    "    ind_genotypes = []\n",
    "    for geno in parts[1:]:\n",
    "        if geno.strip() in new_coding_dict:\n",
    "            ind_genotypes.append(new_coding_dict[geno.strip()])\n",
    "        else:\n",
    "            ind_genotypes.append([0, 0])\n",
    "\n",
    "    current_chunk.append(ind_genotypes)\n",
    "\n",
    "    # If chunk is full, write to disk and clear memory\n",
    "    if len(current_chunk) >= chunk_size:\n",
    "        chunk_filename = os.path.join(temp_dir, f\"genotype_chunk_{chunk_counter}.pk\")\n",
    "        with open(chunk_filename, 'wb') as f:\n",
    "            pk.dump(current_chunk, f)\n",
    "\n",
    "        chunk_files.append(chunk_filename)\n",
    "        chunk_counter += 1\n",
    "        current_chunk = []\n",
    "        gc.collect()  # Force garbage collection\n",
    "\n",
    "# Save last chunk if not empty\n",
    "if current_chunk:\n",
    "    chunk_filename = os.path.join(temp_dir, f\"genotype_chunk_{chunk_counter}.pk\")\n",
    "    with open(chunk_filename, 'wb') as f:\n",
    "        pk.dump(current_chunk, f)\n",
    "    chunk_files.append(chunk_filename)\n",
    "\n",
    "# Close genotype file to free resources\n",
    "genotype_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                 Type            Size (bytes)    Size (MB) \n",
      "------------------------------------------------------------\n",
      "_                    list            8527459484      8132.42   \n",
      "_23                  list            8527459484      8132.42   \n",
      "chunk_data           list            8461130104      8069.16   \n",
      "ind_genotypes        list            8000984         7.63      \n",
      "parts                list            5002505         4.77      \n",
      "out_dict             dict            2405175         2.29      \n",
      "_20                  dict            2405175         2.29      \n",
      "_oh                  dict            2313336         2.21      \n",
      "Out                  dict            2313336         2.21      \n",
      "header               str             788935          0.75      \n",
      "line                 str             200049          0.19      \n",
      "_ih                  list            14499           0.01      \n",
      "In                   list            14499           0.01      \n",
      "_i10                 str             2157            0.00      \n",
      "_i16                 str             1768            0.00      \n",
      "x                    list            1766            0.00      \n",
      "_i24                 str             1720            0.00      \n",
      "_i1                  str             1519            0.00      \n",
      "_i2                  str             1519            0.00      \n",
      "_i14                 str             1519            0.00      \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('_', 'list', 8527459484, 8132.419094085693),\n",
       " ('_23', 'list', 8527459484, 8132.419094085693),\n",
       " ('chunk_data', 'list', 8461130104, 8069.162467956543),\n",
       " ('ind_genotypes', 'list', 8000984, 7.630332946777344),\n",
       " ('parts', 'list', 5002505, 4.770760536193848),\n",
       " ('out_dict', 'dict', 2405175, 2.2937536239624023),\n",
       " ('_20', 'dict', 2405175, 2.2937536239624023),\n",
       " ('_oh', 'dict', 2313336, 2.2061691284179688),\n",
       " ('Out', 'dict', 2313336, 2.2061691284179688),\n",
       " ('header', 'str', 788935, 0.7523870468139648),\n",
       " ('line', 'str', 200049, 0.1907815933227539),\n",
       " ('_ih', 'list', 14499, 0.013827323913574219),\n",
       " ('In', 'list', 14499, 0.013827323913574219),\n",
       " ('_i10', 'str', 2157, 0.0020570755004882812),\n",
       " ('_i16', 'str', 1768, 0.00168609619140625),\n",
       " ('x', 'list', 1766, 0.0016841888427734375),\n",
       " ('_i24', 'str', 1720, 0.00164031982421875),\n",
       " ('_i1', 'str', 1519, 0.0014486312866210938),\n",
       " ('_i2', 'str', 1519, 0.0014486312866210938),\n",
       " ('_i14', 'str', 1519, 0.0014486312866210938),\n",
       " ('_i15', 'str', 1046, 0.0009975433349609375),\n",
       " ('_i12', 'str', 1032, 0.00098419189453125),\n",
       " ('chunk_files', 'list', 994, 0.0009479522705078125),\n",
       " ('row_phenos', 'list', 912, 0.0008697509765625),\n",
       " ('new_coding_dict', 'dict', 412, 0.000392913818359375),\n",
       " ('_i17', 'str', 392, 0.00037384033203125),\n",
       " ('_i19', 'str', 392, 0.00037384033203125),\n",
       " ('phen_file', 'TextIOWrapper', 216, 0.00020599365234375),\n",
       " ('genotype_file', 'TextIOWrapper', 216, 0.00020599365234375),\n",
       " ('_dh', 'list', 168, 0.00016021728515625),\n",
       " ('f', 'BufferedReader', 168, 0.00016021728515625),\n",
       " ('file_path', 'PosixPath', 104, 9.918212890625e-05),\n",
       " ('_i18', 'str', 103, 9.822845458984375e-05),\n",
       " ('_i5', 'str', 101, 9.632110595703125e-05),\n",
       " ('_i8', 'str', 101, 9.632110595703125e-05),\n",
       " ('_i11', 'str', 101, 9.632110595703125e-05),\n",
       " ('_i13', 'str', 98, 9.34600830078125e-05),\n",
       " ('_i3', 'str', 95, 9.059906005859375e-05),\n",
       " ('chunk_filename', 'str', 81, 7.724761962890625e-05),\n",
       " ('chunk_file', 'str', 81, 7.724761962890625e-05),\n",
       " ('_i4', 'str', 77, 7.343292236328125e-05),\n",
       " ('_i6', 'str', 77, 7.343292236328125e-05),\n",
       " ('os', 'module', 72, 6.866455078125e-05),\n",
       " ('np', 'module', 72, 6.866455078125e-05),\n",
       " ('pk', 'module', 72, 6.866455078125e-05),\n",
       " ('h5py', 'module', 72, 6.866455078125e-05),\n",
       " ('gc', 'module', 72, 6.866455078125e-05),\n",
       " ('sys', 'module', 72, 6.866455078125e-05),\n",
       " ('_iii', 'str', 62, 5.91278076171875e-05),\n",
       " ('_i21', 'str', 62, 5.91278076171875e-05),\n",
       " ('temp_dir', 'str', 61, 5.817413330078125e-05),\n",
       " ('_i', 'str', 57, 5.435943603515625e-05),\n",
       " ('y', 'str', 57, 5.435943603515625e-05),\n",
       " ('_i23', 'str', 57, 5.435943603515625e-05),\n",
       " ('_i7', 'str', 56, 5.340576171875e-05),\n",
       " ('_i9', 'str', 56, 5.340576171875e-05),\n",
       " ('current_chunk', 'list', 56, 5.340576171875e-05),\n",
       " ('_21', 'list', 56, 5.340576171875e-05),\n",
       " ('_ii', 'str', 51, 4.863739013671875e-05),\n",
       " ('_i22', 'str', 51, 4.863739013671875e-05),\n",
       " ('_i20', 'str', 49, 4.673004150390625e-05),\n",
       " ('_3', 'File', 48, 4.57763671875e-05),\n",
       " ('h5f', 'File', 48, 4.57763671875e-05),\n",
       " ('geno', 'str', 42, 4.00543212890625e-05),\n",
       " ('_13', 'bool', 28, 2.6702880859375e-05),\n",
       " ('_15', 'int', 28, 2.6702880859375e-05),\n",
       " ('chunk_size', 'int', 28, 2.6702880859375e-05),\n",
       " ('chunk_counter', 'int', 28, 2.6702880859375e-05)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "def list_memory_objects(limit=20):\n",
    "    \"\"\"\n",
    "    List objects in memory along with their size.\n",
    "\n",
    "    Args:\n",
    "        limit: Number of largest objects to display\n",
    "    \"\"\"\n",
    "    # Get all objects in the global namespace\n",
    "    objects = globals().items()\n",
    "\n",
    "    # Calculate size of each object\n",
    "    object_sizes = []\n",
    "    for name, obj in objects:\n",
    "        # Skip modules, functions, and other non-data objects\n",
    "        if name.startswith('__') or callable(obj) or name == 'list_memory_objects':\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            size = sys.getsizeof(obj)\n",
    "            # For numpy arrays, get actual memory usage\n",
    "            if isinstance(obj, np.ndarray):\n",
    "                size = obj.nbytes\n",
    "            # For lists, add size of elements\n",
    "            elif isinstance(obj, list) and obj:\n",
    "                size += sum(sys.getsizeof(i) for i in obj)\n",
    "            # For dictionaries, add size of keys and values\n",
    "            elif isinstance(obj, dict) and obj:\n",
    "                size += sum(sys.getsizeof(k) + sys.getsizeof(v) for k, v in obj.items())\n",
    "\n",
    "            object_sizes.append((name, type(obj).__name__, size, size / (1024 * 1024)))\n",
    "        except:\n",
    "            object_sizes.append((name, type(obj).__name__, 0, 0))\n",
    "\n",
    "    # Sort by size (largest first)\n",
    "    object_sizes.sort(key=itemgetter(2), reverse=True)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{'Name':<20} {'Type':<15} {'Size (bytes)':<15} {'Size (MB)':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    for name, type_name, size, size_mb in object_sizes[:limit]:\n",
    "        print(f\"{name:<20} {type_name:<15} {size:<15} {size_mb:<10.2f}\")\n",
    "\n",
    "    return object_sizes\n",
    "\n",
    "# Example usage\n",
    "list_memory_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now combine all chunks for final output\n",
    "out_dict['genotypes'] = []\n",
    "for chunk_file in chunk_files:\n",
    "    with open(chunk_file, 'rb') as f:\n",
    "        chunk_data = pk.load(f)\n",
    "        out_dict['genotypes'].extend(chunk_data)\n",
    "\n",
    "    # Delete chunk file after reading to free disk space\n",
    "    os.remove(chunk_file)\n",
    "\n",
    "# Remove temporary directory\n",
    "os.rmdir(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_hdf5(out_dict, 'test_sim_WF_1kbt_100kups_5mb_full.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "def inspect_hdf5_file(hdf5_path: Path):\n",
    "    \"\"\"\n",
    "    Safely inspect an HDF5 file to check for structural issues\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(hdf5_path, \"r\") as h5f:\n",
    "            # Print the top-level keys\n",
    "            print(\"Top-level groups/datasets:\", list(h5f.keys()))\n",
    "\n",
    "            # Examine each top-level key without fully loading the data\n",
    "            for key in h5f.keys():\n",
    "                try:\n",
    "                    item = h5f[key]\n",
    "                    if isinstance(item, h5py.Group):\n",
    "                        print(f\"Group '{key}' contains: {list(item.keys())}\")\n",
    "                    elif isinstance(item, h5py.Dataset):\n",
    "                        print(f\"Dataset '{key}' has shape: {item.shape} and dtype: {item.dtype}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error accessing '{key}': {str(e)}\")\n",
    "\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening file: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = out_dict\n",
    "\n",
    "out_dict_test = {}\n",
    "out_dict_train = {}\n",
    "\n",
    "categories_to_stratefy = ['phenotypes', 'genotypes', 'strain_names']\n",
    "categories_to_copy = [x for x in in_data.keys() if x not in categories_to_stratefy]\n",
    "\n",
    "train_length = round(len(in_data['strain_names'])*0.85)\n",
    "\n",
    "#train set\n",
    "for x in categories_to_copy:\n",
    " out_dict_train[x] = in_data[x]\n",
    "\n",
    "for x in categories_to_stratefy:\n",
    " out_dict_train[x] = in_data[x][:train_length]\n",
    "\n",
    "#pk.dump(out_dict_train, open('gpatlas/' + file_prefix + '_train.pk','wb'))\n",
    "save_to_hdf5(out_dict_train, 'test_sim_WF_1kbt_100kups_5mb_train.h5')\n",
    "\n",
    "del(out_dict_train)\n",
    "\n",
    "#test set\n",
    "for x in categories_to_copy:\n",
    " out_dict_test[x] = in_data[x]\n",
    "\n",
    "for x in categories_to_stratefy:\n",
    " out_dict_test[x] = in_data[x][train_length:]\n",
    "\n",
    "#pk.dump(out_dict_test, open('gpatlas/' + file_prefix + '_test.pk','wb'))\n",
    "save_to_hdf5(out_dict_test, 'test_sim_WF_1kbt_100kups_5mb_test.h5')\n",
    "\n",
    "del(out_dict_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpatlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
