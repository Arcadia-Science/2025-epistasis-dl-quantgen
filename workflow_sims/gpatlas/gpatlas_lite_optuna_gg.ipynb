{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import pickle as pk\n",
    "import sys\n",
    "import time as tm\n",
    "import argparse\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import FeatureAblation\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from typing import Iterator, Optional\n",
    "import optuna\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import cast\n",
    "import h5py\n",
    "import pickle\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "batch_size = 50\n",
    "num_workers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optuna.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_latent_space = 300\n",
    "num_epochs = 1\n",
    "n_phen = 25\n",
    "\n",
    "############\n",
    "\n",
    "n_geno = 100000\n",
    "n_alleles = 2\n",
    "latent_space_g = 1000\n",
    "num_epochs_gen = 1\n",
    "\n",
    "############\n",
    "\n",
    "gp_latent_space = p_latent_space\n",
    "epochs_gen_phen = 2\n",
    "\n",
    "l1_lambda = 0.00000000000001\n",
    "l2_lambda = 0.00000000000001\n",
    "\n",
    "#l1_lambda = 0.08\n",
    "#l2_lambda = 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pickle_to_hdf5(pickle_path: Path, hdf5_path: Path, gzip: bool = True) -> Path:\n",
    "    data = pickle.load(open(pickle_path, \"rb\"))\n",
    "    str_dt = h5py.string_dtype(encoding=\"utf-8\")\n",
    "\n",
    "    with h5py.File(hdf5_path, \"w\") as h5f:\n",
    "        metadata_group = h5f.create_group(\"metadata\")\n",
    "\n",
    "        loci_array = np.array(data[\"loci\"], dtype=str_dt)\n",
    "        metadata_group.create_dataset(\"loci\", data=loci_array)\n",
    "\n",
    "        pheno_names_array = np.array(data[\"phenotype_names\"], dtype=str_dt)\n",
    "        metadata_group.create_dataset(\"phenotype_names\", data=pheno_names_array)\n",
    "\n",
    "        strains_group = h5f.create_group(\"strains\")\n",
    "\n",
    "        for idx, strain_id in enumerate(data[\"strain_names\"]):\n",
    "            strain_grp = strains_group.create_group(strain_id)\n",
    "\n",
    "            pheno = np.array(data[\"phenotypes\"][idx], dtype=np.float64)\n",
    "            strain_grp.create_dataset(\"phenotype\", data=pheno)\n",
    "\n",
    "            genotype = np.array(data[\"genotypes\"][idx], dtype=np.int8)\n",
    "            strain_grp.create_dataset(\n",
    "                \"genotype\",\n",
    "                data=genotype,\n",
    "                chunks=True,\n",
    "                compression=\"gzip\" if gzip else None,\n",
    "            )\n",
    "\n",
    "        print(f\"{hdf5_path} generated from {pickle_path}.\")\n",
    "\n",
    "    return hdf5_path\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, hdf5_path: Path) -> None:\n",
    "        self.h5 = h5py.File(hdf5_path, \"r\")\n",
    "\n",
    "        self._strain_group = cast(h5py.Group, self.h5[\"strains\"])\n",
    "        self.strains: list[str] = list(self._strain_group.keys())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._strain_group)\n",
    "\n",
    "\n",
    "class GenoPhenoDataset(BaseDataset):\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        strain = self.strains[idx]\n",
    "\n",
    "        strain_data = cast(Dataset, self._strain_group[strain])\n",
    "\n",
    "        # Note: genotype is being cast as float32 here, reasons not well understood.\n",
    "        phens = torch.tensor(strain_data[\"phenotype\"][:], dtype=torch.float32)\n",
    "        gens = torch.tensor(strain_data[\"genotype\"][:], dtype=torch.float32).flatten()\n",
    "\n",
    "        return phens, gens\n",
    "\n",
    "class PhenoDataset(BaseDataset):\n",
    "    def __getitem__(self, idx: int):\n",
    "        strain = self.strains[idx]\n",
    "\n",
    "        strain_data = cast(Dataset, self._strain_group[strain])\n",
    "\n",
    "        # Note: genotype is being cast as float32 here, reasons not well understood.\n",
    "        phens = torch.tensor(strain_data[\"phenotype\"][:], dtype=torch.float32)\n",
    "\n",
    "\n",
    "        return phens\n",
    "###########\n",
    "class GenoDataset(BaseDataset):\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        strain = self.strains[idx]\n",
    "\n",
    "        strain_data = cast(Dataset, self._strain_group[strain])\n",
    "\n",
    "        # Note: genotype is being cast as float32 here, reasons not well understood.\n",
    "        gens = torch.tensor(strain_data[\"genotype\"][:], dtype=torch.float32).flatten()\n",
    "\n",
    "        return  gens\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    parser = argparse.ArgumentParser(description=\"Convert a Dave's pickle data to an HDF5 file.\")\n",
    "#    parser.add_argument(\"pickle_path\", type=Path, help=\"Path to the input pickle file.\")\n",
    "#    parser.add_argument(\"hdf5_path\", type=Path, help=\"Path to the output HDF5 file.\")\n",
    "#    parser.add_argument(\"gzip\", type=bool, help=\"Gzip datasets (decreases read speed).\")\n",
    "#    args = parser.parse_args()\n",
    "\n",
    "    #convert_pickle_to_hdf5(args.pickle_path, args.hdf5_path, args.gzip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_pickle_to_hdf5('gpatlas/test_sim_WF_1kbt_10000n_5000000bp_test.pk', 'gpatlas/test_sim_WF_1kbt_10000n_5000000bp_test.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pheno = PhenoDataset('gpatlas/test_sim_WF_1kbt_10000n_5000000bp_train.hdf5')\n",
    "test_data_pheno = PhenoDataset('gpatlas/test_sim_WF_1kbt_10000n_5000000bp_test.hdf5')\n",
    "\n",
    "train_data_geno = GenoDataset('gpatlas/test_sim_WF_1kbt_10000n_5000000bp_train.hdf5')\n",
    "test_data_geno = GenoDataset('gpatlas/test_sim_WF_1kbt_10000n_5000000bp_test.hdf5')\n",
    "\n",
    "train_data_gp = GenoPhenoDataset('gpatlas/test_sim_WF_1kbt_10000n_5000000bp_train.hdf5')\n",
    "test_data_gp = GenoPhenoDataset('gpatlas/test_sim_WF_1kbt_10000n_5000000bp_test.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader_pheno = torch.utils.data.DataLoader(\n",
    "    dataset=train_data_pheno, batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
    ")\n",
    "test_loader_pheno = torch.utils.data.DataLoader(\n",
    "    dataset=test_data_pheno, batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "train_loader_geno = torch.utils.data.DataLoader(\n",
    "    dataset=train_data_geno, batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
    ")\n",
    "test_loader_geno = torch.utils.data.DataLoader(\n",
    "    dataset=test_data_geno, batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "train_loader_gp = torch.utils.data.DataLoader(\n",
    "    dataset=train_data_gp, batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
    ")\n",
    "test_loader_gp = torch.utils.data.DataLoader(\n",
    "    dataset=test_data_gp, batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gencoder\n",
    "class GQ_net(nn.Module):\n",
    "    def __init__(self, n_loci=None, N=None):\n",
    "        super().__init__()\n",
    "        if N is None:\n",
    "            N = latent_space_g\n",
    "        if n_loci is None:\n",
    "            n_loci = n_geno * n_alleles\n",
    "\n",
    "        batchnorm_momentum = 0.8\n",
    "        g_latent_dim = latent_space_g\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=n_loci, out_features=N),\n",
    "            nn.BatchNorm1d(N, momentum=0.8),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(in_features=N, out_features=g_latent_dim),\n",
    "            nn.BatchNorm1d(g_latent_dim, momentum=0.8),\n",
    "            nn.LeakyReLU(0.01),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# gendecoder\n",
    "class GP_net(nn.Module):\n",
    "    def __init__(self, n_loci=None, N=None):\n",
    "        super().__init__()\n",
    "        if N is None:\n",
    "            N = latent_space_g\n",
    "        if n_loci is None:\n",
    "            n_loci = n_geno * n_alleles\n",
    "\n",
    "        batchnorm_momentum = 0.8\n",
    "        g_latent_dim = latent_space_g\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=g_latent_dim, out_features=N),\n",
    "            nn.BatchNorm1d(N, momentum=batchnorm_momentum),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(in_features=N, out_features=n_loci),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial: optuna.Trial,\n",
    "             train_loader,\n",
    "             test_loader,\n",
    "             n_geno: int,\n",
    "             n_alleles: int,\n",
    "             device: torch.device) -> float:\n",
    "\n",
    "    # Hyperparameters to optimize\n",
    "    #global latent_space_g  # Make it global so the classes can access it\n",
    "    latent_space_g = trial.suggest_int('latent_space_g', 5, 10)\n",
    "    gen_noise = trial.suggest_float('gen_noise', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 1, 2)\n",
    "\n",
    "    # Constants\n",
    "    EPS = 1e-15\n",
    "    adam_b = (0.5, 0.999)\n",
    "    n_loci = n_geno * n_alleles\n",
    "\n",
    "    # Initialize models\n",
    "    GQ = GQ_net(n_loci=n_loci).to(device)\n",
    "    GP = GP_net(n_loci=n_loci).to(device)\n",
    "\n",
    "    # Optimizers\n",
    "    optim_GQ_enc = torch.optim.Adam(GQ.parameters(), lr=learning_rate, betas=adam_b)\n",
    "    optim_GP_dec = torch.optim.Adam(GP.parameters(), lr=learning_rate, betas=adam_b)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        GQ.train()\n",
    "        GP.train()\n",
    "\n",
    "        epoch_losses = []\n",
    "\n",
    "        for i, gens in enumerate(train_loader):\n",
    "            batch_size = gens.shape[0]\n",
    "\n",
    "            GP.zero_grad()\n",
    "            GQ.zero_grad()\n",
    "\n",
    "            #gens = gens.reshape(gens.shape[0], -1)\n",
    "            #gens = gens[:, :n_loci]\n",
    "\n",
    "            # Apply noise\n",
    "            noise_prob = 1 - gen_noise\n",
    "            pos_noise = np.random.binomial(1, noise_prob / 2, gens.shape)\n",
    "            neg_noise = np.random.binomial(1, noise_prob / 2, gens.shape)\n",
    "\n",
    "            noise_gens = torch.tensor(\n",
    "                np.where((gens + pos_noise - neg_noise) > 0, 1, 0),\n",
    "                dtype=torch.float32\n",
    "            ).to(device)\n",
    "\n",
    "            gens = gens.to(device)\n",
    "\n",
    "            z_sample = GQ(noise_gens)\n",
    "            X_sample = GP(z_sample)\n",
    "\n",
    "            g_recon_loss = F.binary_cross_entropy(X_sample + EPS, gens + EPS)\n",
    "\n",
    "            # L1 and L2 regularization\n",
    "            l1_reg = torch.linalg.norm(torch.sum(GQ.encoder[0].weight, axis=0), 1)\n",
    "            l2_reg = torch.linalg.norm(torch.sum(GQ.encoder[0].weight, axis=0), 2)\n",
    "\n",
    "            total_loss = g_recon_loss + l1_reg * 0 + l2_reg * 0\n",
    "\n",
    "            total_loss.backward()\n",
    "            optim_GQ_enc.step()\n",
    "            optim_GP_dec.step()\n",
    "\n",
    "            epoch_losses.append(float(total_loss.detach()))\n",
    "\n",
    "        # Test set evaluation\n",
    "        if epoch % 1 == 0:\n",
    "            GQ.eval()\n",
    "            GP.eval()\n",
    "            test_losses = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for gens in test_loader:\n",
    "                    #gens = gens.reshape(gens.shape[0], -1)\n",
    "                    gens = gens.to(device)\n",
    "                    #gens = gens[:, :n_loci].to(device)\n",
    "\n",
    "                    z_sample = GQ(gens)\n",
    "                    X_sample = GP(z_sample)\n",
    "\n",
    "                    test_loss = F.binary_cross_entropy(X_sample + EPS, gens + EPS)\n",
    "                    test_losses.append(float(test_loss))\n",
    "\n",
    "            avg_test_loss = np.mean(test_losses)\n",
    "            print(f\"Epoch {epoch}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "    return avg_test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Run optimization\n",
    "n_trials = 1  # Start small for testing\n",
    "try:\n",
    "    study.optimize(\n",
    "        lambda trial: objective(\n",
    "            trial=trial,\n",
    "            train_loader=train_loader_geno,\n",
    "            test_loader=test_loader_geno,\n",
    "            n_geno=n_geno,\n",
    "            n_alleles=n_alleles,\n",
    "            device=device,\n",
    "            #num_epochs=1\n",
    "        ),\n",
    "        n_trials=n_trials\n",
    "    )\n",
    "\n",
    "    print(\"\\nStudy completed!\")\n",
    "    print(f\"Best parameters found: {study.best_params}\")\n",
    "    print(f\"Best value achieved: {study.best_value}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error details: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Run optimization\n",
    "n_trials = 1  # Start small for testing\n",
    "\n",
    "try:\n",
    "    # Create a list to store results as we go\n",
    "    trial_results = []\n",
    "\n",
    "    study.optimize(\n",
    "        lambda trial: objective(\n",
    "            trial=trial,\n",
    "            train_loader=train_loader_geno,\n",
    "            test_loader=test_loader_geno,\n",
    "            n_geno=n_geno,\n",
    "            n_alleles=n_alleles,\n",
    "            device=device\n",
    "        ),\n",
    "        n_trials=n_trials,\n",
    "        callbacks=[\n",
    "            lambda study, trial: trial_results.append({\n",
    "                'trial_number': trial.number,\n",
    "                'params': trial.params,\n",
    "                'value': trial.value,\n",
    "                'state': trial.state.name\n",
    "            })\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"\\nStudy completed!\")\n",
    "    print(f\"Best parameters found: {study.best_params}\")\n",
    "    print(f\"Best value achieved: {study.best_value}\")\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(trial_results)\n",
    "    results_df.to_csv(f'gpatlas/optuna/optuna_trials_{timestamp}.csv', index=False)\n",
    "\n",
    "    # Save detailed study information to JSON\n",
    "    study_info = {\n",
    "        'best_params': study.best_params,\n",
    "        'best_value': study.best_value,\n",
    "        'n_trials': n_trials,\n",
    "        'datetime': timestamp,\n",
    "        'all_trials': trial_results\n",
    "    }\n",
    "\n",
    "    with open(f'gpatlas/optuna/optuna_study_{timestamp}.json', 'w') as f:\n",
    "        json.dump(study_info, f, indent=4)\n",
    "\n",
    "    print(f\"\\nResults saved to:\")\n",
    "    print(f\"- optuna_trials_{timestamp}.csv\")\n",
    "    print(f\"- optuna_study_{timestamp}.json\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error details: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "    # Save error information\n",
    "    with open(f'optuna_error_{timestamp}.txt', 'w') as f:\n",
    "        f.write(f\"Error occurred: {str(e)}\\n\")\n",
    "        traceback.print_exc(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geno_encodings = []\n",
    "genos = []\n",
    "geno_latent = []\n",
    "\n",
    "# Set models to eval mode\n",
    "GQ.eval()\n",
    "GP.eval()\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for gt in test_loader_geno:\n",
    "        # Move data to device\n",
    "        gt = gt.to(device)\n",
    "\n",
    "        # Forward pass through autoencoder\n",
    "        z_sample = GQ(gt)  # Encode\n",
    "        X_sample = GP(z_sample)  # Decode\n",
    "\n",
    "        # Store results\n",
    "        genos.append(gt.cpu().numpy())\n",
    "        geno_encodings.append(X_sample.cpu().numpy())\n",
    "        geno_latent.append(z_sample.cpu().numpy())\n",
    "\n",
    "        # Clean up memory\n",
    "        del z_sample, X_sample\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Concatenate all batches and transpose\n",
    "genos = np.concatenate(genos, axis=0).T\n",
    "geno_encodings = np.concatenate(geno_encodings, axis=0).T\n",
    "geno_latent = np.concatenate(geno_latent, axis=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f1_scores = []\n",
    "\n",
    "# Set models to eval mode\n",
    "GQ.eval()\n",
    "GP.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for gt in test_loader_geno:\n",
    "        gt = gt.to(device)\n",
    "\n",
    "        # Forward pass through autoencoder\n",
    "        z_sample = GQ(gt)\n",
    "        X_sample = GP(z_sample)\n",
    "\n",
    "        # Move to CPU and convert to numpy\n",
    "        original = gt.cpu().numpy()\n",
    "        reconstructed = X_sample.cpu().numpy()\n",
    "\n",
    "        # Reshape to [batch_size, n_loci, 2] to separate alleles\n",
    "        original = original.reshape(-1, n_geno, 2)\n",
    "        reconstructed = reconstructed.reshape(-1, n_geno, 2)\n",
    "\n",
    "        # Take just the first allele state for each locus\n",
    "        original_allele1 = original[:, :, 0]\n",
    "        reconstructed_allele1 = (reconstructed[:, :, 0] > 0.5).astype(int)\n",
    "\n",
    "        # Calculate F1 score for each sample in the batch\n",
    "        for orig, recon in zip(original_allele1, reconstructed_allele1):\n",
    "            f1 = f1_score(orig, recon, average='macro')\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "# Plot distribution of F1 scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(f1_scores, bins=50)\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of F1 Scores for First Allele State Reconstruction')\n",
    "plt.axvline(np.mean(f1_scores), color='r', linestyle='dashed', label=f'Mean F1: {np.mean(f1_scores):.3f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.3f} ± {np.std(f1_scores):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpatlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
